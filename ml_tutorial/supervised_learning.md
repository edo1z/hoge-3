# 教師あり学習 - ラベル付きデータからの学習

教師あり学習（Supervised Learning）は、機械学習の最も基本的かつ広く使われているアプローチです。このタイプの学習では、「正解（ラベル）」付きのデータからパターンを学習し、未知のデータに対して予測を行います。この章では、教師あり学習の基本的な概念から実践的な応用までを、超初心者にも分かりやすく解説します。

## 1. 教師あり学習の基本概念

### 教師あり学習とは

教師あり学習は、「入力データ」と「正解（出力）」のペアを使ってモデルを訓練する手法です。この「教師（正解ラベル）」によって、モデルは入力と出力の関係を学習し、新しい未知の入力に対して適切な出力を予測できるようになります。

### 簡単な例で理解する

例えば、Eメールがスパムかどうかを判定する問題を考えてみましょう：

- **入力データ（X）**: Eメールの特徴（件名、本文の単語、送信者など）
- **正解ラベル（y）**: スパム（1）または非スパム（0）

モデルは多くのラベル付きEメールから学習し、新しいEメールがスパムかどうかを予測できるようになります。

### 教師あり学習の一般的なワークフロー

1. **データ収集**: ラベル付きのデータセットを用意する
2. **データ前処理**: データをクリーニング、正規化、特徴量エンジニアリングする
3. **データ分割**: データを訓練セットとテストセット（および検証セット）に分ける
4. **モデルの選択**: 問題に適したアルゴリズムを選ぶ
5. **モデルの訓練**: 訓練データを使ってモデルを学習させる
6. **モデルの評価**: テストデータでモデルの性能を評価する
7. **モデルのチューニング**: ハイパーパラメータを調整して性能を向上させる
8. **モデルの展開**: 学習したモデルを実際のアプリケーションで使用する

## 2. 教師あり学習の主なタスク

教師あり学習には、主に2つのタイプのタスクがあります：

### 分類（Classification）

分類は、入力データを予め定義されたカテゴリ（クラス）に割り当てるタスクです。

#### 二値分類

2つのクラスに分類する問題です。
- **例**: スパムメール検出（スパム/非スパム）、疾病診断（陽性/陰性）、不正検出（詐欺/非詐欺）

#### 多クラス分類

3つ以上のクラスに分類する問題です。
- **例**: 手書き数字認識（0-9の10クラス）、動物の画像分類（犬、猫、鳥など）、文書のトピック分類

#### 多ラベル分類

1つのデータが複数のクラスに同時に属する可能性がある問題です。
- **例**: 映画のジャンル分類（「アクション」かつ「コメディ」）、画像のタグ付け（「海」「夕日」「船」）

### 回帰（Regression）

回帰は、連続的な数値を予測するタスクです。

#### 単回帰

1つの特徴量から目標変数を予測する問題です。
- **例**: 土地の広さから家の価格を予測する

#### 重回帰

複数の特徴量から目標変数を予測する問題です。
- **例**: 家の広さ、築年数、場所などから価格を予測する

#### 非線形回帰

特徴量と目標変数の関係が線形でない問題です。
- **例**: 株価の予測、人口増加の予測

## 3. 代表的な教師あり学習アルゴリズム

### 線形モデル

#### 線形回帰（Linear Regression）

連続値を予測するための最も基本的なアルゴリズムです。

**基本概念**:
- 入力変数と出力変数の間に線形関係があると仮定します
- $y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$ という形式でモデル化します
- 最小二乗法などを使って最適なパラメータ（$w$と$b$）を求めます

**長所**:
- シンプルで解釈しやすい
- 計算が高速
- 少ないデータでも学習可能

**短所**:
- 非線形関係をモデル化できない
- 外れ値に敏感

**適用例**:
- 家の価格予測
- 売上予測
- 温度予測

#### ロジスティック回帰（Logistic Regression）

分類問題に使用される線形モデルです。

**基本概念**:
- 線形回帰の出力にシグモイド関数（$\sigma(z) = \frac{1}{1+e^{-z}}$）を適用
- 出力が0〜1の範囲になり、確率として解釈できる
- 閾値（通常は0.5）以上なら1、未満なら0と分類

**長所**:
- シンプルで解釈しやすい
- クラスの確率も提供
- 過学習しにくい

**短所**:
- 複雑な非線形の決定境界を表現できない
- 特徴量間の相互作用を自動で考慮しない

**適用例**:
- スパム検出
- 顧客のコンバージョン予測
- 医療診断

### 決定木ベースのモデル

#### 決定木（Decision Trees）

ツリー構造を使って、質問を順次行い、答えを予測します。

**基本概念**:
- 特徴量に基づいて質問（分岐）を繰り返し、葉ノードで予測を行う
- 情報利得やジニ不純度を基準に最適な分岐を選択
- 視覚的に理解しやすい構造を持つ

**長所**:
- 解釈が非常に容易
- 非線形関係も扱える
- 特徴量のスケーリングが不要

**短所**:
- 過学習しやすい
- 単一の木では予測精度が低いことが多い
- 不安定（データの小さな変化で木の構造が大きく変わる）

**適用例**:
- 与信判断
- 医療診断
- 顧客のセグメンテーション

#### ランダムフォレスト（Random Forest）

複数の決定木を組み合わせたアンサンブルモデルです。

**基本概念**:
- ブートストラップサンプリング（データの重複を許した抽出）で多様な訓練データを作成
- 各ノードでランダムに選ばれた特徴量のサブセットから最適な分岐を選択
- 複数の木の予測を集計（平均や多数決）して最終予測とする

**長所**:
- 高い予測精度
- 過学習しにくい
- 特徴量の重要度を算出可能

**短所**:
- 個々の決定木ほど解釈しやすくない
- 計算コストが高い
- モデルサイズが大きい

**適用例**:
- 顧客離反予測
- 医療画像診断
- 金融リスク評価

#### 勾配ブースティング（Gradient Boosting）

弱い学習器（通常は浅い決定木）を逐次的に組み合わせるアンサンブル手法です。

**基本概念**:
- 前の学習器の誤差を次の学習器が修正するように学習を進める
- 各学習器は残差（実際の値と予測値の差）を予測するように訓練される
- 代表的な実装：XGBoost、LightGBM、CatBoost

**長所**:
- 非常に高い予測精度
- さまざまな問題に適用可能
- ハイパーパラメータの調整で過学習をコントロール可能

**短所**:
- チューニングが複雑
- 計算コストが高い
- ブラックボックス的な側面がある

**適用例**:
- 検索ランキング
- 広告のクリック率予測
- Kaggleコンペティションで頻繁に使用される

### サポートベクターマシン（SVM）

データを高次元空間に写像し、クラスを分離する超平面を見つけるアルゴリズムです。

**基本概念**:
- クラス間のマージンを最大化する超平面を探す
- カーネルトリック（kernel trick）を使って非線形分類も可能
- サポートベクター（決定境界に最も近いデータ点）が重要

**長所**:
- 高次元データに有効
- メモリ効率が良い（サポートベクターのみを保存）
- 多様なカーネル関数で様々な問題に対応

**短所**:
- 大規模データセットでは計算コストが高い
- パラメータチューニングが難しい
- 確率出力が直接得られない

**適用例**:
- テキスト分類
- 画像認識
- 生体情報の分類

### k近傍法（k-Nearest Neighbors）

新しいデータポイントを、訓練データセットの中で最も類似した k 個のデータポイントの多数決で分類します。

**基本概念**:
- 予測したいデータポイントから最も近い k 個の訓練データを見つける
- 分類：多数決でクラスを決定
- 回帰：k個の平均値を予測値とする

**長所**:
- シンプルで実装が容易
- 訓練フェーズが不要（怠惰学習）
- 非線形な決定境界も表現可能

**短所**:
- 予測が遅い（全訓練データとの距離計算が必要）
- 特徴量のスケールに敏感
- 次元の呪い（高次元では距離の概念が薄れる）

**適用例**:
- レコメンデーションシステム
- 画像認識
- 異常検知

### ニューラルネットワーク

人間の脳の構造を模倣した、層状のニューロンから構成されるモデルです。

**基本概念**:
- 入力層、隠れ層、出力層からなる
- 各ニューロンは重み付き入力の和に活性化関数を適用
- 誤差逆伝播法で重みを調整する

**長所**:
- 複雑なパターンを学習可能
- 画像、テキスト、音声などの様々なデータに適用可能
- ディープラーニングの基盤技術

**短所**:
- 多くの訓練データが必要
- 計算コストが高い
- ブラックボックス的で解釈が難しい

**適用例**:
- 画像認識
- 自然言語処理
- 音声認識

## 4. モデル評価とバリデーション

### 学習と汎化

- **学習（訓練）**: モデルが訓練データのパターンを覚えること
- **汎化**: 学習したモデルが未知のデータに対しても正しく予測できること
- **過学習（オーバーフィッティング）**: 訓練データに過度に適合し、未知データでの性能が低下すること
- **過少学習（アンダーフィッティング）**: モデルが単純すぎてデータのパターンを捉えられていないこと

### データの分割

モデルの評価には、データを複数のセットに分割することが重要です：

- **訓練データ（Training Data）**: モデルの学習に使用（通常60-80%）
- **検証データ（Validation Data）**: ハイパーパラメータの調整に使用（通常10-20%）
- **テストデータ（Test Data）**: 最終的なモデル評価に使用（通常10-20%）

### 交差検証（Cross-Validation）

限られたデータを効率的に使うための手法です。

**k分割交差検証（k-fold Cross-Validation）**:
1. データを k 個の等しいサイズの「フォールド」に分割
2. k回の反復で、各回異なるフォールドをテストデータとし、残りを訓練データとする
3. k回の評価結果の平均をモデルの性能とする

**長所**:
- データを効率的に使用できる
- より安定した性能評価が可能
- モデルの安定性を確認できる

### 評価指標

#### 分類問題の評価指標

**混同行列（Confusion Matrix）**:
- 真陽性（TP）: 正例を正しく正例と予測
- 真陰性（TN）: 負例を正しく負例と予測
- 偽陽性（FP）: 負例を誤って正例と予測（第一種の誤り）
- 偽陰性（FN）: 正例を誤って負例と予測（第二種の誤り）

**精度（Accuracy）**:
- 全体の予測のうち、正しい予測の割合
- $(TP + TN) / (TP + TN + FP + FN)$
- バランスの取れたデータセットに適している

**適合率（Precision）**:
- 正例と予測したもののうち、実際に正例である割合
- $TP / (TP + FP)$
- 偽陽性を最小化したい場合に重要（例：スパムフィルタ）

**再現率（Recall）または感度（Sensitivity）**:
- 実際の正例のうち、正しく正例と予測された割合
- $TP / (TP + FN)$
- 偽陰性を最小化したい場合に重要（例：疾病スクリーニング）

**F1スコア**:
- 適合率と再現率の調和平均
- $2 \times (Precision \times Recall) / (Precision + Recall)$
- バランスの取れた評価が必要な場合に有用

**ROC曲線とAUC**:
- 様々な閾値における真陽性率vs偽陽性率のグラフ
- AUC（曲線下面積）はモデルの全体的な性能を表す
- 1に近いほど良い分類器（0.5はランダム予測と同等）

#### 回帰問題の評価指標

**平均絶対誤差（MAE）**:
- 予測値と実際の値の差の絶対値の平均
- $\frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$
- 解釈が簡単で外れ値の影響を受けにくい

**平均二乗誤差（MSE）**:
- 予測値と実際の値の差の二乗の平均
- $\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
- 大きな誤差に対してペナルティが大きい

**二乗平均平方根誤差（RMSE）**:
- MSEの平方根
- $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$
- 元の目標変数と同じ単位で解釈可能

**決定係数（R²）**:
- モデルによって説明される分散の割合
- $1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$
- 1に近いほど良いモデル（1が完璧な予測、0がベースラインモデルと同等）

## 5. ハイパーパラメータチューニング

### ハイパーパラメータとは

ハイパーパラメータは、学習アルゴリズム自体を制御するパラメータで、学習プロセスの前に設定する必要があります。

例：
- 決定木の最大深さ
- k近傍法の k の値
- ニューラルネットワークの層数やニューロン数
- 正則化パラメータ

### チューニング手法

#### グリッドサーチ（Grid Search）

- 可能なハイパーパラメータの組み合わせをすべて試す
- 計算コストが高いが、確実に最適な組み合わせを見つけられる

#### ランダムサーチ（Random Search）

- ハイパーパラメータの組み合わせをランダムにサンプリングして試す
- グリッドサーチより効率的なことが多い

#### ベイズ最適化（Bayesian Optimization）

- 過去の試行結果に基づいて、次に試すべきハイパーパラメータを予測
- 効率的に最適なハイパーパラメータを見つけられる

## 6. 実践的なPythonによる教師あり学習の例

以下に、Pythonを使った教師あり学習の簡単な例を示します。

### 分類問題の例：アイリスデータセット

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# データの読み込み
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# モデルの訓練
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 予測
y_pred = model.predict(X_test)

# 評価
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred, target_names=target_names)

print(f"精度: {accuracy:.2f}")
print("\n混同行列:")
print(conf_matrix)
print("\n分類レポート:")
print(class_report)

# 特徴量の重要度を可視化
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("特徴量の重要度")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()
```

### 回帰問題の例：ボストン住宅価格データセット

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# データの読み込み
housing = fetch_california_housing()
X = housing.data
y = housing.target
feature_names = housing.feature_names

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 線形回帰モデルの訓練
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# 勾配ブースティング回帰モデルの訓練
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

# 予測
y_pred_linear = linear_model.predict(X_test)
y_pred_gb = gb_model.predict(X_test)

# 評価
mse_linear = mean_squared_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print("線形回帰モデル:")
print(f"MSE: {mse_linear:.4f}")
print(f"RMSE: {np.sqrt(mse_linear):.4f}")
print(f"R²: {r2_linear:.4f}")

print("\n勾配ブースティングモデル:")
print(f"MSE: {mse_gb:.4f}")
print(f"RMSE: {np.sqrt(mse_gb):.4f}")
print(f"R²: {r2_gb:.4f}")

# 実際の値と予測値の比較
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_gb, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('実際の値')
plt.ylabel('予測値')
plt.title('勾配ブースティングモデルの予測値 vs 実際の値')
plt.tight_layout()
plt.show()

# 特徴量の重要度を可視化
importances = gb_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("特徴量の重要度")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()
```

## 7. 教師あり学習の実際の応用例

### 現実世界での応用例

#### 医療診断

- **問題**: 医療画像（X線、MRIなど）から病気を診断する
- **アプローチ**: CNNなどのディープラーニングモデルを使用
- **特徴量**: 画像のピクセル値、抽出された画像特徴
- **メリット**: 医師の診断を支援し、見落としを減らせる

#### 信用リスク評価

- **問題**: ローン申請者の返済能力を予測する
- **アプローチ**: 勾配ブースティングやランダムフォレストなどを使用
- **特徴量**: 収入、年齢、職業、過去の信用履歴など
- **メリット**: 公平で一貫した信用評価が可能

#### 顧客離反予測

- **問題**: どの顧客が解約する可能性が高いかを予測する
- **アプローチ**: ロジスティック回帰やランダムフォレストなどを使用
- **特徴量**: 使用頻度、顧客満足度、サービス利用期間など
- **メリット**: 予防的な対策で顧客維持率を向上できる

### 実装する際の課題と解決策

#### データ品質の問題

- **課題**: 欠損値、外れ値、不均衡なクラス分布
- **解決策**:
  - 適切なデータクリーニングと前処理
  - オーバーサンプリングやアンダーサンプリングでクラスバランスを調整
  - 適切な特徴量エンジニアリング

#### モデルの選択

- **課題**: 問題に最適なモデルを選ぶのが難しい
- **解決策**:
  - 複数のモデルをベンチマークする
  - モデルの説明可能性と精度のトレードオフを考慮
  - 問題の制約（計算リソース、解釈性の要件など）を考慮

#### 過学習への対処

- **課題**: 複雑なモデルが訓練データに過剰適合する
- **解決策**:
  - 正則化手法（L1、L2正則化など）の使用
  - データの拡張
  - 早期停止（early stopping）の使用
  - アンサンブル手法の活用

## まとめ

教師あり学習は、ラベル付きデータからパターンを学習し、未知のデータに対して予測を行う強力な機械学習アプローチです。

**主なポイント**:
- 教師あり学習は分類と回帰の2つの主要なタスクに対応
- アルゴリズムの選択は問題の性質、データ量、解釈性の要件などによって決まる
- 過学習を防ぎ、汎化性能を高めることが重要
- モデル評価は適切な評価指標と検証手法で行う
- 実世界での応用は、データの前処理から始まり、モデルの展開と監視まで含む包括的なプロセス

次の章では、[教師なし学習](unsupervised_learning.md)について詳しく見ていきましょう。これは、ラベルのないデータからパターンを見つけ出す機械学習の別のアプローチです。
