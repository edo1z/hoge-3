# 線形代数入門 - 機械学習のための数学基礎

機械学習では、データを**ベクトル**や**行列**として扱うことが多いため、線形代数の基礎知識は非常に重要です。この章では、線形代数の基本概念を、高校数学レベルから分かりやすく解説します。

## 1. スカラー、ベクトル、行列、テンソル

### スカラー (Scalar)

スカラーは単なる数値です。例えば、温度、年齢、価格などは全てスカラー値です。

例：
- 気温: 25℃
- 商品価格: 1,000円
- 年齢: 30歳

### ベクトル (Vector)

ベクトルは、複数の数値が順序を持って並んだものです。機械学習では、**特徴量**を表現するのによく使われます。

例：ある人の特徴を表すベクトル
```
x = [身長, 体重, 年齢]
x = [170, 65, 30]
```

数学的な表記では、ベクトルは通常、太字の小文字で表します：
$$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$$

ここで、$x_i$ はベクトルの $i$ 番目の要素（成分）です。

#### ベクトルの次元

ベクトルが持つ要素の数を「次元」と呼びます。
- 2次元ベクトル: $\mathbf{x} = [x_1, x_2]$
- 3次元ベクトル: $\mathbf{x} = [x_1, x_2, x_3]$

#### ベクトルの解釈方法

ベクトルは以下のように解釈できます：
1. **方向と大きさ**: 原点から特定の方向に伸びる矢印
2. **座標**: 空間上の点の座標
3. **特徴の集合**: 機械学習ではデータの特徴量を表現

### 行列 (Matrix)

行列は、数値が縦横に並んだ2次元の表です。機械学習では、複数のデータサンプル（例：複数の人の特徴）をまとめて扱うときに使います。

例：3人の特徴（身長、体重、年齢）を表す行列
```
X = [[170, 65, 30],  # 1人目
     [160, 55, 25],  # 2人目
     [180, 75, 35]]  # 3人目
```

数学的な表記では、行列は通常、太字の大文字で表します：
$$\mathbf{X} = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mn}
\end{bmatrix}$$

ここで、$x_{ij}$ は行列の $i$ 行 $j$ 列目の要素です。

#### 行列のサイズ

行列のサイズは「行数×列数」で表します。
- $2 \times 3$ 行列: 2行3列の行列
- $3 \times 2$ 行列: 3行2列の行列

### テンソル (Tensor)

テンソルは、スカラー（0次元）、ベクトル（1次元）、行列（2次元）を一般化した、多次元の配列です。深層学習では、画像や動画などの複雑なデータを表現するのに使われます。

例：カラー画像（幅×高さ×色チャネル）
```
画像テンソル = [[[R値, G値, B値], ...], ...]
```

## 2. ベクトルの基本演算

### ベクトルの加算

同じ次元のベクトル同士は、対応する要素を足し合わせることで加算できます。

例：
$$\mathbf{a} = [a_1, a_2, a_3]$$
$$\mathbf{b} = [b_1, b_2, b_3]$$
$$\mathbf{a} + \mathbf{b} = [a_1 + b_1, a_2 + b_2, a_3 + b_3]$$

具体例：
$$[1, 2, 3] + [4, 5, 6] = [5, 7, 9]$$

### ベクトルのスカラー倍

ベクトルの各要素にスカラー値を掛けることで、ベクトル全体を拡大・縮小できます。

例：
$$\alpha \mathbf{a} = [\alpha \cdot a_1, \alpha \cdot a_2, \alpha \cdot a_3]$$

具体例：
$$2 \cdot [1, 2, 3] = [2, 4, 6]$$

### ベクトルの内積（ドット積）

二つのベクトルの内積は、対応する要素同士を掛けて、その結果を全て足し合わせたスカラー値です。

例：
$$\mathbf{a} \cdot \mathbf{b} = a_1 \times b_1 + a_2 \times b_2 + a_3 \times b_3 = \sum_{i=1}^{n} a_i b_i$$

具体例：
$$[1, 2, 3] \cdot [4, 5, 6] = 1 \times 4 + 2 \times 5 + 3 \times 6 = 4 + 10 + 18 = 32$$

#### 内積の意味

内積は二つのベクトルの「類似度」を表します：
- 内積が大きい：二つのベクトルは似ている（同じ方向を向いている）
- 内積が0：二つのベクトルは直交している（無関係）
- 内積が負：二つのベクトルは逆方向を向いている

これは機械学習でのデータの類似度計算などに利用されます。

### ベクトルのノルム（長さ）

ベクトルのノルムは、ベクトルの「大きさ」や「長さ」を表します。一般的には、ユークリッドノルム（L2ノルム）を使います。

$$||\mathbf{a}|| = \sqrt{a_1^2 + a_2^2 + \cdots + a_n^2} = \sqrt{\sum_{i=1}^{n} a_i^2}$$

具体例：
$$||[3, 4]|| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$$

これは平面上で、点(0,0)から点(3,4)までの距離が5であることを表しています（ピタゴラスの定理！）。

## 3. 行列の基本演算

### 行列の加算

同じサイズの行列同士は、対応する要素を足し合わせることで加算できます。

例：
$$\mathbf{A} + \mathbf{B} =
\begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12} \\
a_{21} + b_{21} & a_{22} + b_{22}
\end{bmatrix}$$

具体例：
$$\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} +
\begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} =
\begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}$$

### 行列のスカラー倍

行列の各要素にスカラー値を掛けることで、行列全体を拡大・縮小できます。

例：
$$\alpha \mathbf{A} =
\begin{bmatrix}
\alpha \cdot a_{11} & \alpha \cdot a_{12} \\
\alpha \cdot a_{21} & \alpha \cdot a_{22}
\end{bmatrix}$$

具体例：
$$2 \cdot \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} =
\begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}$$

### 行列の乗算

行列の乗算は少し複雑です。行列 $\mathbf{A}$ (m×n) と行列 $\mathbf{B}$ (n×p) の積 $\mathbf{C} = \mathbf{A} \times \mathbf{B}$ は、m×p の行列になります。

$\mathbf{C}$ の各要素 $c_{ij}$ は、$\mathbf{A}$ の $i$ 行と $\mathbf{B}$ の $j$ 列の内積として計算されます：

$$c_{ij} = \sum_{k=1}^{n} a_{ik} \cdot b_{kj}$$

例：
$$\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \times
\begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} =
\begin{bmatrix}
1 \times 5 + 2 \times 7 & 1 \times 6 + 2 \times 8 \\
3 \times 5 + 4 \times 7 & 3 \times 6 + 4 \times 8
\end{bmatrix} =
\begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}$$

重要ポイント：
- 行列の乗算は、左側の行列の列数と右側の行列の行数が一致している必要があります
- 行列の乗算は、一般的に交換法則が成り立ちません（$\mathbf{A} \times \mathbf{B} \neq \mathbf{B} \times \mathbf{A}$）

### 単位行列

単位行列 $\mathbf{I}$ は、対角線上の要素が全て1で、それ以外の要素が全て0の正方行列です。

例：$2 \times 2$ の単位行列
$$\mathbf{I} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$

単位行列の特性：
- 任意の行列 $\mathbf{A}$ について、$\mathbf{A} \times \mathbf{I} = \mathbf{I} \times \mathbf{A} = \mathbf{A}$ となります
- これは実数の「1」と同じような役割を果たします

### 転置行列

行列 $\mathbf{A}$ の転置行列 $\mathbf{A}^T$ は、$\mathbf{A}$ の行と列を入れ替えたものです。

例：
$$\mathbf{A} = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$$

$$\mathbf{A}^T = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$$

### 逆行列

正方行列 $\mathbf{A}$ の逆行列 $\mathbf{A}^{-1}$ は、$\mathbf{A} \times \mathbf{A}^{-1} = \mathbf{A}^{-1} \times \mathbf{A} = \mathbf{I}$ となる行列です。

逆行列は行列方程式を解くのに重要です：
$$\mathbf{A} \times \mathbf{X} = \mathbf{B}$$
$$\mathbf{X} = \mathbf{A}^{-1} \times \mathbf{B}$$

全ての正方行列に逆行列があるわけではありません。逆行列が存在する行列を「正則（非特異）行列」と呼びます。

## 4. 機械学習での線形代数の応用例

### 線形回帰での行列演算

線形回帰では、以下の行列方程式を解きます：
$$\mathbf{X} \times \mathbf{w} = \mathbf{y}$$

ここで：
- $\mathbf{X}$ は特徴量の行列
- $\mathbf{w}$ は重みベクトル
- $\mathbf{y}$ は目標値のベクトル

最小二乗法による解は：
$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

### データの変換と射影

主成分分析（PCA）などの次元削減手法では、データを低次元空間に射影するために行列演算が使われます：
$$\mathbf{Z} = \mathbf{X} \times \mathbf{W}$$

ここで：
- $\mathbf{X}$ は元のデータ行列
- $\mathbf{W}$ は射影行列
- $\mathbf{Z}$ は低次元表現

### ニューラルネットワークでの行列演算

ニューラルネットワークの各層の計算は行列演算で表現されます：
$$\mathbf{H} = f(\mathbf{X} \times \mathbf{W} + \mathbf{b})$$

ここで：
- $\mathbf{X}$ は入力データ
- $\mathbf{W}$ は重み行列
- $\mathbf{b}$ はバイアスベクトル
- $f$ は活性化関数
- $\mathbf{H}$ は出力（次の層への入力）

## 5. Pythonでの実装例

NumPyを使うと、ベクトルや行列の演算を簡単に実装できます：

```python
import numpy as np

# ベクトル
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# ベクトルの加算
print(a + b)  # [5 7 9]

# ベクトルの内積
print(np.dot(a, b))  # 32

# ベクトルのノルム（長さ）
print(np.linalg.norm(a))  # 3.7416573867739413

# 行列
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# 行列の加算
print(A + B)
# [[ 6  8]
#  [10 12]]

# 行列の乗算
print(np.dot(A, B))
# [[19 22]
#  [43 50]]

# 単位行列
I = np.eye(2)
print(I)
# [[1. 0.]
#  [0. 1.]]

# 転置行列
print(A.T)
# [[1 3]
#  [2 4]]

# 逆行列
print(np.linalg.inv(A))
# [[-2.   1. ]
#  [ 1.5 -0.5]]
```

## まとめ

線形代数は機械学習の根幹をなす数学的基盤です。この章で学んだ概念：

- **スカラー、ベクトル、行列、テンソル**: データの表現方法
- **ベクトルの演算**: 加算、スカラー倍、内積、ノルム
- **行列の演算**: 加算、乗算、転置、逆行列
- **機械学習での応用**: 線形回帰、次元削減、ニューラルネットワーク

これらの概念は、次の章で学ぶ[確率・統計の基礎](probability_statistics.md)と合わせて、機械学習アルゴリズムを理解する上で非常に重要です。