# 確率・統計の基礎 - 機械学習のための数学

機械学習は、データからパターンを見つけ出し、予測や分類を行う技術です。そのため、データの特性を理解するための確率・統計の知識は非常に重要です。この章では、機械学習に必要な確率と統計の基礎概念を、高校数学レベルから分かりやすく解説します。

## 1. 基本統計量 - データを数値で要約する

### 平均 (Mean)

平均は、データの中心的な値を表す最も基本的な統計量です。すべての値を足して、データの数で割ることで計算します。

数式：
$$\mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$

例：
```
データ: [2, 4, 6, 8, 10]
平均: (2 + 4 + 6 + 8 + 10) ÷ 5 = 30 ÷ 5 = 6
```

#### 平均の特徴
- データの「中心」を表す
- 外れ値（極端に大きいまたは小さい値）の影響を受けやすい
- 機械学習では、データの正規化やモデルの評価に使用される

### 中央値 (Median)

中央値は、データを小さい順に並べたときに真ん中に来る値です。データの数が偶数の場合は、真ん中の2つの値の平均を取ります。

例：
```
データ: [2, 4, 6, 8, 10]
並べ替え: [2, 4, 6, 8, 10]
中央値: 6（5つの中の3番目）

データ: [2, 4, 6, 8]
並べ替え: [2, 4, 6, 8]
中央値: (4 + 6) ÷ 2 = 5（4つの中の2番目と3番目の平均）
```

#### 中央値の特徴
- 外れ値の影響を受けにくい
- データが偏っている（歪んでいる）場合は、平均よりも代表値として適切なことが多い

### 分散 (Variance)

分散は、データが平均からどれだけばらついているかを表す指標です。各データ点と平均との差の二乗の平均で計算します。

数式：
$$\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2$$

例：
```
データ: [2, 4, 6, 8, 10]
平均: 6
各点と平均の差: [-4, -2, 0, 2, 4]
差の二乗: [16, 4, 0, 4, 16]
分散: (16 + 4 + 0 + 4 + 16) ÷ 5 = 40 ÷ 5 = 8
```

#### 分散の特徴
- 値が大きいほど、データのばらつきが大きい
- 単位が元のデータの単位の二乗になる（例：身長cmの分散はcm²）
- 機械学習では、特徴の重要度や正規化に使用される

### 標準偏差 (Standard Deviation)

標準偏差は、分散の平方根です。分散と同様にデータのばらつきを表しますが、元のデータと同じ単位で表されるため、より直感的に理解しやすい指標です。

数式：
$$\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}$$

例：
```
データ: [2, 4, 6, 8, 10]
分散: 8
標準偏差: √8 ≈ 2.83
```

#### 標準偏差の特徴
- 元のデータと同じ単位で表される
- 正規分布では、平均±標準偏差の範囲に約68%のデータが含まれる
- 機械学習では、異常検知や特徴のスケーリングに使用される

### パーセンタイル (Percentile)

パーセンタイルは、データを小さい順に並べたときに、特定の割合の位置にある値です。例えば、50パーセンタイルは中央値と同じです。

例：
```
データ: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
25パーセンタイル (Q1): 6（下位25%の最大値）
50パーセンタイル (中央値): 11（下位50%の最大値）
75パーセンタイル (Q3): 16（下位75%の最大値）
```

#### パーセンタイルの特徴
- データの分布の様々な部分を理解するのに役立つ
- 四分位範囲（IQR = Q3 - Q1）は、データの中央50%の広がりを表す
- 機械学習では、外れ値の検出や箱ひげ図の作成に使用される

## 2. 確率の基礎概念

### 確率とは

確率とは、ある事象が起こる可能性を0から1の間の数値で表したものです。0は絶対に起こらないことを、1は必ず起こることを意味します。

例：
- 公平なコインの表が出る確率: 1/2 = 0.5
- サイコロで6の目が出る確率: 1/6 ≈ 0.167
- トランプからハートのエースを引く確率: 1/52 ≈ 0.019

### 確率の性質

1. 全ての可能な事象の確率の合計は1である
   例: サイコロの目が1, 2, 3, 4, 5, 6のいずれかである確率は1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1

2. どんな事象の確率も0以上1以下である
   例: 0 ≤ P(事象A) ≤ 1

3. 排反事象（同時に起こり得ない事象）の確率は足し合わせることができる
   例: サイコロで偶数の目が出る確率 = P(2) + P(4) + P(6) = 1/6 + 1/6 + 1/6 = 1/2

### 条件付き確率

条件付き確率とは、ある事象が起きたという条件のもとで、別の事象が起きる確率です。

数式：
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

ここで、$P(A|B)$は「Bが起きたという条件のもとでのAの確率」、$P(A \cap B)$は「AとB両方が起きる確率」を表します。

例：
```
袋に赤玉3個と青玉2個が入っている場合：
1. 最初に引いた玉が赤である確率: 3/5 = 0.6
2. 最初に赤玉を引いた後、次も赤玉を引く確率: 2/4 = 0.5
   (最初に1個取り出した後は全部で4個になるため)
```

### ベイズの定理

ベイズの定理は、条件付き確率の関係を表す定理で、機械学習の多くのアルゴリズム（特にナイーブベイズ分類器）の基礎となっています。

数式：
$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$

例：
```
あるがんの検査の例：
- がんである確率（有病率）: P(がん) = 0.01 (1%)
- がんの人が陽性反応を示す確率（感度）: P(陽性|がん) = 0.9 (90%)
- がんでない人が陰性反応を示す確率（特異度）: P(陰性|がんでない) = 0.8 (80%)
- がんでない人が陽性反応を示す確率: P(陽性|がんでない) = 0.2 (20%)

検査で陽性だった場合、実際にがんである確率は？

P(がん|陽性) = (P(陽性|がん) × P(がん)) / P(陽性)
P(陽性) = P(陽性|がん) × P(がん) + P(陽性|がんでない) × P(がんでない)
        = 0.9 × 0.01 + 0.2 × 0.99 = 0.009 + 0.198 = 0.207

したがって、
P(がん|陽性) = (0.9 × 0.01) / 0.207 ≈ 0.043 (約4.3%)
```

このように、検査で陽性だったとしても、実際にがんである確率は約4.3%にしかなりません。これは、ベイズの定理を使って計算された「事後確率」です。

## 3. 確率分布 - データの広がり方

### 確率分布とは

確率分布は、確率変数（ランダムな値を取る変数）が取り得る値とその確率の関係を表すものです。

#### 離散確率分布
離散確率分布は、取り得る値が離散的（バラバラ）な確率分布です。

例：
- コイン投げの結果（表/裏）
- サイコロの目（1～6）
- 家族の子供の人数（0, 1, 2, ...）

#### 連続確率分布
連続確率分布は、取り得る値が連続的な確率分布です。

例：
- 身長や体重
- 気温
- テストの点数（理論上は連続値として扱える場合）

### 代表的な確率分布

#### 1. 二項分布 (Binomial Distribution)

二項分布は、「成功」と「失敗」の2つの結果しかない試行（ベルヌーイ試行）を繰り返したときの「成功」の回数の分布です。

数式：
$$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$$

ここで：
- $n$ は試行回数
- $k$ は成功回数
- $p$ は各試行での成功確率
- $\binom{n}{k}$ は組み合わせ（nCk）

例：
```
コインを5回投げたとき、表が3回出る確率：
n = 5, k = 3, p = 0.5
P(X = 3) = 5C3 × 0.5³ × 0.5² = 10 × 0.125 × 0.25 = 0.3125
```

#### 2. 正規分布 (Normal Distribution / Gaussian Distribution)

正規分布は、自然界の多くの現象に現れる連続確率分布で、釣り鐘型の対称な形をしています。平均 $\mu$ と標準偏差 $\sigma$ の2つのパラメータで特徴づけられます。

数式：
$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

ここで：
- $\mu$ は平均
- $\sigma$ は標準偏差
- $e$ はネイピア数（約2.71828...）
- $\pi$ は円周率（約3.14159...）

#### 正規分布の特徴
- 平均・中央値・最頻値が全て同じ値
- 平均を中心に左右対称
- 平均±1σの範囲に約68%、±2σの範囲に約95%、±3σの範囲に約99.7%のデータが含まれる（68-95-99.7法則）
- 機械学習の多くの手法では、データが正規分布に従うと仮定している

#### 3. ポアソン分布 (Poisson Distribution)

ポアソン分布は、一定時間内に発生する事象の回数の分布を表します。例えば、1時間あたりの来客数、ウェブサイトの1日あたりのアクセス数などに適用されます。

数式：
$$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

ここで：
- $\lambda$ は単位時間あたりの平均発生回数
- $k$ は発生回数
- $e$ はネイピア数
- $k!$ は階乗

例：
```
平均して1時間に5人の客が来るカフェで、1時間に8人の客が来る確率：
λ = 5, k = 8
P(X = 8) = 5⁸ × e⁻⁵ / 8! ≈ 0.0655
```

#### 4. 一様分布 (Uniform Distribution)

一様分布は、特定の範囲内のどの値も等しい確率で出現する分布です。

連続一様分布の数式：
$$f(x) = \frac{1}{b-a}, \quad \text{for } a \leq x \leq b$$

ここで、$[a, b]$ は値の範囲です。

例：
```
1から6までの公平なサイコロでは、各目が出る確率は1/6で均等
```

## 4. 統計的推測 - サンプルから全体を推測する

### 母集団とサンプル

- **母集団**（Population）: 研究対象全体のこと（例：日本のすべての成人）
- **サンプル**（Sample）: 母集団から抽出された一部（例：調査に回答した1000人の日本人成人）

実際には母集団全体を調査することは難しいため、サンプルを使って母集団の特性を推測します。

### 標本統計量と母数

- **標本統計量**: サンプルから計算される統計量（標本平均、標本分散など）
- **母数**: 母集団の特性を表すパラメータ（母平均、母分散など）

統計的推測では、標本統計量を使って母数を推定します。

### 信頼区間

信頼区間は、母数（例：母平均）が特定の確率でその区間内に含まれると推測される範囲です。

例：平均の95%信頼区間
$$\bar{x} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}$$

ここで：
- $\bar{x}$ は標本平均
- $\sigma$ は母標準偏差（未知の場合は標本標準偏差 $s$ で代用）
- $n$ はサンプルサイズ
- 1.96は95%信頼水準に対応する標準正規分布の臨界値

### 統計的仮説検定

仮説検定は、データに基づいて仮説の真偽を判定する方法です。

基本的なステップ：
1. 帰無仮説（H₀）と対立仮説（H₁）を設定
2. 検定統計量を計算
3. p値を計算
4. 有意水準（通常5%）と比較して、帰無仮説を棄却するかどうかを決定

例：
```
新薬の効果を検証する場合：
H₀: 新薬はプラセボと効果に差がない
H₁: 新薬はプラセボより効果がある

p値が0.03の場合、有意水準5%で帰無仮説を棄却し、
「新薬はプラセボより効果がある」と結論付ける
```

#### p値とは
p値は、「帰無仮説が正しいという前提のもとで、観測されたデータと同じかそれ以上に極端な結果が得られる確率」です。p値が小さいほど、帰無仮説を棄却する根拠が強くなります。

## 5. 機械学習における確率・統計の応用

### データの前処理

- **標準化（Standardization）**: データを平均0、標準偏差1に変換
  $$z = \frac{x - \mu}{\sigma}$$

- **正規化（Normalization）**: データを特定の範囲（通常は0～1）に変換
  $$x_{norm} = \frac{x - \min(x)}{\max(x) - \min(x)}$$

### モデル評価

- **バイアス-バリアンス トレードオフ**: モデルの複雑さと汎化能力のバランス
- **交差検証（Cross-validation）**: データを複数の部分に分割し、モデルの性能を評価
- **精度（Accuracy）、適合率（Precision）、再現率（Recall）、F1スコア**: 分類モデルの評価指標

### 確率モデル

- **ナイーブベイズ分類器**: ベイズの定理に基づく分類アルゴリズム
- **ロジスティック回帰**: 結果が特定のカテゴリに属する確率をモデル化
- **最尤推定（Maximum Likelihood Estimation）**: データが観測される確率を最大にするパラメータを推定

## 6. Pythonでの実装例

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# 基本統計量
data = [2, 5, 7, 9, 12, 15, 18, 22, 25, 30]
mean = np.mean(data)
median = np.median(data)
variance = np.var(data)
std_dev = np.std(data)

print(f"平均: {mean}")
print(f"中央値: {median}")
print(f"分散: {variance}")
print(f"標準偏差: {std_dev}")

# ヒストグラムで分布を可視化
plt.figure(figsize=(10, 6))
plt.hist(data, bins=5, edgecolor='black', alpha=0.7)
plt.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'平均: {mean:.2f}')
plt.axvline(median, color='green', linestyle='dashed', linewidth=1, label=f'中央値: {median:.2f}')
plt.title('データの分布')
plt.xlabel('値')
plt.ylabel('頻度')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 正規分布の例
x = np.linspace(-4, 4, 1000)
y_normal = stats.norm.pdf(x, 0, 1)  # 標準正規分布 (平均0, 標準偏差1)

plt.figure(figsize=(10, 6))
plt.plot(x, y_normal, label='標準正規分布')
plt.fill_between(x, y_normal, where=((x >= -1) & (x <= 1)), color='skyblue', alpha=0.4,
                 label='平均±1σ (約68%)')
plt.fill_between(x, y_normal, where=((x >= -2) & (x <= 2)), color='blue', alpha=0.1,
                 label='平均±2σ (約95%)')
plt.title('正規分布と68-95-99.7法則')
plt.xlabel('標準偏差の単位')
plt.ylabel('確率密度')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 二項分布の例
n = 10  # 試行回数
p = 0.3  # 成功確率
k = np.arange(0, n+1)  # 成功回数の可能な値
binomial = stats.binom.pmf(k, n, p)  # 二項分布の確率質量関数

plt.figure(figsize=(10, 6))
plt.bar(k, binomial, edgecolor='black')
plt.title(f'二項分布 (n={n}, p={p})')
plt.xlabel('成功回数')
plt.ylabel('確率')
plt.grid(True, alpha=0.3)
plt.show()
```

## まとめ

確率・統計は機械学習の根幹をなす数学的基盤です。この章で学んだ概念：

- **基本統計量**: 平均、中央値、分散、標準偏差、パーセンタイル
- **確率の基礎**: 条件付き確率、ベイズの定理
- **確率分布**: 二項分布、正規分布、ポアソン分布、一様分布
- **統計的推測**: 母集団とサンプル、信頼区間、仮説検定
- **機械学習への応用**: データ前処理、モデル評価、確率モデル

これらの概念は、次の章で学ぶ[微分の基礎](calculus_basics.md)と合わせて、機械学習アルゴリズムを理解する上で非常に重要です。